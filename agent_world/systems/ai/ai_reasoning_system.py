"""Basic LLM reasoning loop for AI-controlled agents."""

from __future__ import annotations

from typing import Any, List, Optional, Tuple
import re  # For checking hex string pattern
import logging

logger = logging.getLogger(__name__)

COOLDOWN_TICKS = 10 # How many ticks an agent waits after an LLM action before requesting another

from ...core.components.ai_state import AIState
from ...ai.llm.prompt_builder import build_prompt
from ...ai.llm.llm_manager import LLMManager
from ...core.components.role import RoleComponent
from .behavior_tree import BehaviorTree, build_fallback_tree
from .actions import parse_action_string, ActionQueue

# Define strings that are considered non-actions or failures from the LLM
NON_ACTION_STRINGS = [
    "<wait>",
    "<error_llm_call>",
    "<wait_llm_not_ready>",
    "<error_llm_setup>",
    "<error_llm_loop_missing>",
    "<error_llm_queue_full>",
    "<error_llm_no_choices>",
    "<error_llm_malformed_choice>",
    "<error_llm_malformed_message>",
    "<error_llm_malformed_content>",
    "<error_llm_request>",
    "<error_llm_parsing>",
    "<llm_empty_response>", # For when LLM returns an empty string
    "" # Explicitly include empty string
]
# Add HTTP error variations
for i in range(400, 600):
    NON_ACTION_STRINGS.append(f"<error_llm_http_{i}>")

# Precompile a regex for typical prompt_id (UUID hex string)
# Prompt IDs are generated by uuid.uuid4().hex, which are 32 lowercase hex characters
PROMPT_ID_PATTERN = re.compile(r"^[a-f0-9]{32}$")


class AIReasoningSystem:
    """Query the LLM for each agent and queue resulting actions."""

    def __init__(
        self,
        world: Any,
        llm: LLMManager,
        action_tuples_list: List[Tuple[int, str]], # This is world.raw_actions_with_actor
        behavior_tree: Optional[BehaviorTree] = None,
    ) -> None:
        self.world = world
        self.llm = llm
        self.action_tuples_list = action_tuples_list
        self.behavior_tree = behavior_tree or build_fallback_tree()
        self.action_queue: ActionQueue | None = getattr(world, "action_queue", None)
        self._sink_wrapped = isinstance(action_tuples_list, RawActionCollector)

    def update(self, tick: int) -> None: # tick parameter is passed by SystemsManager
        """Handle pending and new LLM prompts for all agents."""

        if not all([
            self.world.entity_manager, 
            self.world.component_manager, 
            self.world.time_manager,
            self.world.llm_manager_instance, 
            hasattr(self.world, 'async_llm_responses')
        ]):
            # print(f"[Tick {tick}][AIReasoningSystem] World or essential managers not ready.")
            return

        em = self.world.entity_manager
        cm = self.world.component_manager
        tm = self.world.time_manager
        
        for entity_id in list(em.all_entities.keys()):
            ai_comp = cm.get_component(entity_id, AIState)
            if ai_comp is None:
                continue

            role_comp = cm.get_component(entity_id, RoleComponent)
            if role_comp and not role_comp.uses_llm:
                if self.behavior_tree:
                    action = self.behavior_tree.run(entity_id, self.world)
                    if action:
                        if not self._sink_wrapped:
                            if self.action_queue is None:
                                self.action_queue = getattr(self.world, "action_queue", None)
                            if self.action_queue is not None:
                                for act in parse_action_string(entity_id, action):
                                    self.action_queue._queue.append(act)
                        self.action_tuples_list.append((entity_id, action))
                continue

            bypass_cooldown = False
            if ai_comp.needs_immediate_rethink:
                ai_comp.needs_immediate_rethink = False
                bypass_cooldown = True

            if (
                not bypass_cooldown
                and tm.tick_counter
                <= ai_comp.last_llm_action_tick + COOLDOWN_TICKS
                and ai_comp.last_llm_action_tick != -1
            ):
                continue

            final_action_to_take: str | None = None
            llm_attempt_made_or_resolved = False # Track if we interacted with LLM this tick

            if ai_comp.pending_llm_prompt_id is None:
                # No pending request, try to make a new one
                if self.llm.mode == "live":
                    llm_attempt_made_or_resolved = True
                    prompt = build_prompt(entity_id, self.world)
                    if role_comp and not role_comp.can_request_abilities:
                        prompt = "\n".join(
                            line for line in prompt.splitlines()
                            if "GENERATE_ABILITY" not in line
                        )
                    returned_value = self.llm.request(prompt, self.world)

                    if returned_value in NON_ACTION_STRINGS:
                        logger.debug(
                            "[Tick %s][AI Agent %s] LLM returned immediate non-action/error: '%s' for prompt: %s...",
                            tm.tick_counter,
                            entity_id,
                            returned_value,
                            prompt[:70],
                        )
                        # final_action_to_take remains None, BT will be tried.
                    elif PROMPT_ID_PATTERN.match(returned_value):
                        # This is a new prompt_id because llm.request scheduled a new call
                        ai_comp.pending_llm_prompt_id = returned_value
                        logger.debug(
                            "[Tick %s][AI Agent %s] New LLM request initiated. Prompt ID: %s. Prompt: %s...",
                            tm.tick_counter,
                            entity_id,
                            ai_comp.pending_llm_prompt_id,
                            prompt[:70],
                        )
                    else:
                        # This is an immediate valid action (e.g., from cache, or echo mode if not live)
                        logger.info(
                            "[Tick %s][AI Agent %s] LLM returned immediate valid action: '%s' for prompt: %s...",
                            tm.tick_counter,
                            entity_id,
                            returned_value,
                            prompt[:70],
                        )
                        final_action_to_take = returned_value
                
                elif self.llm.mode == "echo": # Handle echo mode separately if not covered by "live"
                    llm_attempt_made_or_resolved = True
                    prompt = build_prompt(entity_id, self.world)
                    if role_comp and not role_comp.can_request_abilities:
                        prompt = "\n".join(
                            line for line in prompt.splitlines()
                            if "GENERATE_ABILITY" not in line
                        )
                    returned_action = self.llm.request(prompt, self.world)  # LLMManager handles echo logic
                    logger.debug(
                        "[Tick %s][AI Agent %s] LLM Echo mode response: '%s' for prompt: %s...",
                        tm.tick_counter,
                        entity_id,
                        returned_action,
                        prompt[:70],
                    )
                    if returned_action not in NON_ACTION_STRINGS:
                        final_action_to_take = returned_action
                # If LLM mode is "offline", llm.request returns "<wait>", which is in NON_ACTION_STRINGS,
                # so it will fall through to behavior tree. No specific handling needed here.

            else: # Has a pending LLM request
                llm_attempt_made_or_resolved = True
                future = self.world.async_llm_responses.get(ai_comp.pending_llm_prompt_id)
                if future and future.done():
                    try:
                        action_from_llm = future.result()
                        logger.debug(
                            "[Tick %s][AI Agent %s] LLM Future resolved. Prompt ID %s. Result: '%s'",
                            tm.tick_counter,
                            entity_id,
                            ai_comp.pending_llm_prompt_id,
                            action_from_llm,
                        )
                        if action_from_llm not in NON_ACTION_STRINGS:
                            final_action_to_take = action_from_llm
                    except Exception as e:
                        logger.warning(
                            "[Tick %s][AI Agent %s] Error getting result from LLM future for Prompt ID %s: %s",
                            tm.tick_counter,
                            entity_id,
                            ai_comp.pending_llm_prompt_id,
                            e,
                        )
                    
                    self.world.async_llm_responses.pop(ai_comp.pending_llm_prompt_id, None)
                    ai_comp.pending_llm_prompt_id = None
                # else: Future not done yet, agent waits for next AIReasoningSystem update.

            # If no action from LLM, try behavior tree
            if not final_action_to_take and self.behavior_tree:
                # Only log BT usage if an LLM attempt was made/resolved OR if LLM is offline
                if llm_attempt_made_or_resolved or self.llm.mode == "offline":
                    logger.debug(
                        "[Tick %s][AI Agent %s] No valid LLM action. Mode: %s. Trying behavior tree.",
                        tm.tick_counter,
                        entity_id,
                        self.llm.mode,
                    )
                
                fallback_action = self.behavior_tree.run(entity_id, self.world)
                if fallback_action:
                    final_action_to_take = fallback_action
                    # print(f"[Tick {tm.tick_counter}][AI Agent {entity_id}] Behavior tree provided fallback: '{fallback_action}'")
            
            if final_action_to_take:
                logger.info(
                    "[Tick %s][AI Agent %s] Decided action: '%s' (LLM Mode: %s)",
                    tm.tick_counter,
                    entity_id,
                    final_action_to_take,
                    self.llm.mode,
                )
                if not self._sink_wrapped:
                    if self.action_queue is None:
                        self.action_queue = getattr(self.world, "action_queue", None)
                    if self.action_queue is not None:
                        parsed_actions = parse_action_string(entity_id, final_action_to_take)
                        for act in parsed_actions:
                            self.action_queue._queue.append(act)
                self.action_tuples_list.append((entity_id, final_action_to_take))
                ai_comp.last_llm_action_tick = tm.tick_counter
            elif llm_attempt_made_or_resolved and ai_comp.pending_llm_prompt_id is None: 
                # If an LLM attempt was made/resolved, but resulted in NO action (e.g. LLM error, or LLM returned <wait>),
                # and NO BT action was taken (e.g. BT also returned None, or no BT),
                # still update last_llm_action_tick to enforce cooldown and prevent spamming failed LLM calls.
                # This happens if the future resolved to a NON_ACTION_STRING, or immediate request was NON_ACTION_STRING
                # and behavior tree also didn't yield an action.
                # Or, if the goal is for it to retry LLM immediately on <wait> type errors, then this line should be conditional.
                # Current behavior: cooldown even on failed LLM attempt cycle if BT also fails.
                # print(f"[Tick {tm.tick_counter}][AI Agent {entity_id}] LLM attempt cycle completed with no action, pending_id cleared. Cooldown started.") # DEBUG
                ai_comp.last_llm_action_tick = tm.tick_counter


__all__ = ["AIReasoningSystem", "RawActionCollector"]


class RawActionCollector(list[tuple[int, str]]):
    """List-like collector that also enqueues parsed actions."""

    def __init__(self, action_queue: ActionQueue) -> None:
        super().__init__()
        self.action_queue = action_queue

    def append(self, item: tuple[int, str]) -> None:  # type: ignore[override]
        actor_id, text = item
        if self.action_queue is not None:
            parsed = parse_action_string(actor_id, text)
            for act in parsed:
                self.action_queue._queue.append(act)
        super().append(item)

